import streamlit as st
import pandas as pd
from io import StringIO
import numpy as np

def generate_prompt(
    problem_type, source_type, analysis_goal, data_context,
    train_path, test_path, submit_path, single_path,
    target_col, id_col, time_col,
    models, use_ensemble, tune_hyperparams,
    ts_features, include_corr, graphs,
    save_dir_path
    ):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠã«åŸºã¥ã„ã¦AIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    output_format = "Kaggleå½¢å¼" if source_type == "Kaggleå½¢å¼" else "ä¸€èˆ¬çš„ãªåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆ"
    if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬" and source_type != "Kaggleå½¢å¼":
        output_format = "æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆ"

    # Step 1
    prompt = [
        f"### ä¾é ¼å†…å®¹ï¼šæ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸã€Œ{problem_type}ã€ã®ã€Œ{analysis_goal}ã€",
        "ã“ã‚Œã‹ã‚‰Google Colabç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã®Pythonã‚³ãƒ¼ãƒ‰ã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒ»ãƒã‚¤ãƒ»ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
        "\n# ==================================",
        "# Step 1: ç’°å¢ƒè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿",
        "# ==================================",
        "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
        "!pip install japanize-matplotlib lightgbm shap holidays -q",
        "\n### ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
        "# (pandas, numpy, lightgbm, sklearn, matplotlib, seaborn, shap, holidays, os ãªã©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„)",
        "\n### Google Driveã®ãƒã‚¦ãƒ³ãƒˆ",
        "from google.colab import drive",
        "drive.mount('/content/drive')",
        "\n### ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ä¿å­˜å…ˆã®è¨­å®š",
    ]

    if source_type == "Kaggleå½¢å¼":
        prompt.append(f"train_data_path = '{train_path}'")
        prompt.append(f"test_data_path = '{test_path}'")
        prompt.append(f"submit_template_path = '{submit_path}'")
    else:
        prompt.append(f"file_path = '{single_path}'")
    prompt.append(f"save_folder_path = '{save_dir_path}'")

    prompt.append("\n### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    if source_type == "Kaggleå½¢å¼":
        prompt.append("df_train = pd.read_csv(train_data_path)")
        prompt.append("df_test = pd.read_csv(test_data_path)")
    else:
        prompt.append("df = pd.read_csv(file_path)")

    # Step 2
    prompt.extend([
        "\n# ==================================",
        "# Step 2: ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã®æŠŠæ¡",
        "# ==================================",
        "### ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ åã¨ã‚µãƒ³ãƒ—ãƒ«",
        "ä»¥ä¸‹ã«ã€åˆ†æå¯¾è±¡ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ æ§‹æˆã¨å…ˆé ­æ•°è¡Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã‚’å…ƒã«åˆ†æã‚’é€²ã‚ã¦ãã ã•ã„ã€‚",
        "--- ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ---",
        data_context,
        "--- ã“ã“ã¾ã§ ---",
        f"\nä»Šå›ã®åˆ†æã®ç›®çš„å¤‰æ•°ã¯ `{target_col}` ã§ã™ã€‚"
    ])

    # Step 3
    prompt.extend([
        "\n# ==================================",
        f"# Step 3: {analysis_goal}ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨è©•ä¾¡",
        "# ==================================",
        "### å®Ÿè¡Œã—ã¦ã»ã—ã„ã‚¿ã‚¹ã‚¯:",
    ])
    
    tasks = [f"- **ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ**: `os.makedirs(save_folder_path, exist_ok=True)` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"]
    
    # å‰å‡¦ç†ã‚¿ã‚¹ã‚¯
    if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬":
        task_str = f"- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†**: `{time_col}`åˆ—ã‚’datetimeå‹ã«å¤‰æ›ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        if source_type == "Kaggleå½¢å¼":
            task_str = f"- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†**: `df_train`ã¨`df_test`ã‚’çµåˆã—ã€`{time_col}`åˆ—ã‚’datetimeå‹ã«å¤‰æ›ã—ã¦å…±é€šã®æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        tasks.append(task_str)

        if ts_features:
            feature_tasks = "\n- **æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ**: ä»¥ä¸‹ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
            for feature in ts_features:
                feature_tasks += f"  - {feature}\n"
            tasks.append(feature_tasks)
    else:
        task_str = "- **å‰å‡¦ç†**: ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ¬ æå€¤ã®å¹³å‡å€¤è£œå®Œãªã©ã€åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚"
        if source_type == "Kaggleå½¢å¼":
            task_str = "- **å‰å‡¦ç†**: `df_train`ã¨`df_test`ã‚’ä¸€åº¦çµåˆã—ã€å…±é€šã®å‰å‡¦ç†ï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ¬ æå€¤ã®å¹³å‡å€¤è£œå®Œãªã©ï¼‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ãã®å¾Œã€å†åº¦`train`ã¨`test`ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
        tasks.append(task_str)

    if include_corr:
        tasks.append("- **ç›¸é–¢åˆ†æ**: å‰å‡¦ç†å¾Œã®ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—ã—ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚")

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¿ã‚¹ã‚¯
    if analysis_goal == "è¦å› åˆ†æ":
        tasks.append("- **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: **è§£é‡ˆæ€§ã®é«˜ã„**ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°/ç·šå½¢å›å¸°, æ±ºå®šæœ¨ãªã©ï¼‰ã‚’å„ªå…ˆã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        tasks.append(f"- **è¦å› åˆ†æ**: å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°ã‚„SHAPå€¤ã‚’ä½¿ã„ã€ã€Œã©ã®ç‰¹å¾´é‡ãŒ `{target_col}` ã«æ­£ã¾ãŸã¯è² ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã€ã‚’åˆ†æãƒ»è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚")
    else:
        model_str = "ã€".join(models)
        tasks.append(f"- **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: `{model_str}` ã‚’ä½¿ã£ã¦ã€äºˆæ¸¬ç²¾åº¦ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã¦ãã ã•ã„ã€‚")
        
        if tune_hyperparams and models:
            cv_method = "TimeSeriesSplitã‚’ä½¿ã£ãŸã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³" if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬" else "é€šå¸¸ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³(cv=5)"
            tasks.append(f"- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: GridSearchCVã‚’ä½¿ã„ã€`{models[0]}`ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã¦ãã ã•ã„ã€‚({cv_method})")
        
        if use_ensemble and len(models) > 1:
            tasks.append("- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: å­¦ç¿’ã•ã›ãŸè¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’å¹³å‡ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")

    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¿ã‚¹ã‚¯
    evaluation_tasks = "\n- **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡**: "
    if problem_type == "åˆ†é¡":
        evaluation_tasks += "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚\n  - æ··åŒè¡Œåˆ— (Confusion Matrix)\n  - æ­£è§£ç‡ (Accuracy), é©åˆç‡ (Precision), å†ç¾ç‡ (Recall), F1ã‚¹ã‚³ã‚¢\n  - AUCã‚¹ã‚³ã‚¢"
    elif problem_type == "å›å¸°":
        evaluation_tasks += "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€çµæœã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚\n  - RMSE (Root Mean Squared Error)\n  - MAE (Mean Absolute Error)\n  - R2ã‚¹ã‚³ã‚¢ (æ±ºå®šä¿‚æ•°)"
    elif problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬":
        evaluation_tasks += "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆã¾ãŸã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåŒºé–“ï¼‰ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚\n  - RMSE (Root Mean Squared Error)\n  - MAE (Mean Absolute Error)\n  - MAPE (Mean Absolute Percentage Error)"
    tasks.append(evaluation_tasks)

    # å¯è¦–åŒ–ã‚¿ã‚¹ã‚¯
    graph_tasks = "\n- **å¯è¦–åŒ–**: ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã€`save_folder_path`ã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚\n"
    if "ç‰¹å¾´é‡ã®é‡è¦åº¦ (SHAP)" not in graphs:
        graphs.insert(0, "ç‰¹å¾´é‡ã®é‡è¦åº¦ (SHAP)")
    for graph in graphs:
        graph_tasks += f"  - {graph}\n"
    tasks.append(graph_tasks)

    # å‡ºåŠ›ã‚¿ã‚¹ã‚¯
    if output_format == "Kaggleå½¢å¼":
        tasks.append(f"- **æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ**: `sample_submission.csv`ã®å½¢å¼ã«åˆã‚ã›ã¦ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬çµæœã‚’`os.path.join(save_folder_path, 'submission.csv')`ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚IDåˆ—ã¯`{id_col}`ã§ã™ã€‚")
    elif problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬":
        tasks.append("- **æœªæ¥äºˆæ¸¬**: å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã€æœªæ¥30æœŸé–“ã®äºˆæ¸¬å€¤ã‚’ç®—å‡ºã—ã€å®Ÿç¸¾å€¤ã¨åˆã‚ã›ã¦ã‚°ãƒ©ãƒ•ã«ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚")

    prompt.append("\n".join(tasks))
    prompt.append("\n---\nä»¥ä¸Šã®å†…å®¹ã§ã€Pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
    
    return "\n".join(prompt)

# --- Streamlit ã‚¢ãƒ—ãƒªã®UIè¨­å®š ---
st.set_page_config(page_title="ğŸ¤– AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼", layout="wide")
st.title("ğŸ¤– AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ for Data Analysis")
st.write("ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã‚¿ã‚¹ã‚¯ã‚’AIã«ä¾é ¼ã™ã‚‹ãŸã‚ã®ã€å®Œç’§ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚")

if 'data_context' not in st.session_state:
    st.session_state.data_context = ""

with st.sidebar:
    st.header("âš™ï¸ è¨­å®šé …ç›®")
    
    # âœ¨ FIX: å…¨ã¦ã®UIå¤‰æ•°ã‚’æœ€åˆã«åˆæœŸåŒ–ã—ã€NameErrorã‚’å®Œå…¨ã«é˜²ã
    train_path, test_path, submit_path, single_path = "", "", "", ""
    id_col, time_col, ts_features = "", "", []
    use_ensemble, tune_hyperparams, include_corr = False, False, True

    st.subheader("1. åˆ†æã®ã‚´ãƒ¼ãƒ«ã‚’é¸æŠ")
    analysis_goal = st.radio("ã“ã®åˆ†æã®ä¸»ãªç›®çš„ã¯ï¼Ÿ", ["äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰", "è¦å› åˆ†æ"], horizontal=True)
    problem_type = st.radio("2. åˆ†æã®ç›®çš„ã‚’é¸æŠ", ["åˆ†é¡", "å›å¸°", "æ™‚ç³»åˆ—äºˆæ¸¬"], horizontal=True)

    st.subheader("3. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    source_type = st.radio("ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼", ["å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«", "Kaggleå½¢å¼"], horizontal=True)
    
    st.write("Google Driveå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    if source_type == "Kaggleå½¢å¼":
        train_path = st.text_input("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ (train.csv) ã®ãƒ‘ã‚¹", "/content/drive/MyDrive/kaggle/train.csv")
        test_path = st.text_input("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (test.csv) ã®ãƒ‘ã‚¹", "/content/drive/MyDrive/kaggle/test.csv")
        submit_path = st.text_input("æå‡ºç”¨ã‚µãƒ³ãƒ—ãƒ« (sample_submission.csv) ã®ãƒ‘ã‚¹", "/content/drive/MyDrive/kaggle/sample_submission.csv")
    else: # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«
        single_path = st.text_input("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹", "/content/drive/MyDrive/data/my_data.csv")
    
    st.subheader("4. ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±")
    target_col_default = "y" if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬" else "survived"
    target_col = st.text_input("ç›®çš„å¤‰æ•°ã®åˆ—å", target_col_default)
    
    # âœ¨ FIX: IDåˆ—ã®å…¥åŠ›UIã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ã—ã€å…¨ã¦ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼
    if source_type == "Kaggleå½¢å¼":
        id_col = st.text_input("ID/è­˜åˆ¥å­ã®åˆ—å", "id")
    
    if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬":
        time_col = st.text_input("æ™‚ç³»åˆ—ã‚«ãƒ©ãƒ ï¼ˆæ—¥ä»˜/æ™‚åˆ»ï¼‰ã®åˆ—å", "ds")
    
    uploaded_file_for_context = st.file_uploader(
        "ã“ã“ã«CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¦‚è¦ã‚’è‡ªå‹•ç”Ÿæˆ", type=['csv'],
        help="åˆ†æå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰ã¾ãŸã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆKaggleå½¢å¼ã®å ´åˆï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
    )
    if uploaded_file_for_context:
        try:
            df_context = pd.read_csv(uploaded_file_for_context)
            buffer = StringIO()
            df_context.info(buf=buffer)
            info_str = buffer.getvalue()
            head_str = df_context.head().to_markdown()
            st.session_state.data_context = f"ã€df.info()ã€‘\n{info_str}\n\nã€df.head()ã€‘\n{head_str}"
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    data_context = st.text_area("ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ï¼ˆè‡ªå‹•å…¥åŠ›ï¼‰", value=st.session_state.data_context, height=200)

    st.subheader("5. ãƒ¢ãƒ‡ãƒ«æˆ¦ç•¥")
    default_models = ["LightGBM"] if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬" else ["LightGBM", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°/ç·šå½¢å›å¸°"]
    models = st.multiselect("ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«", ["LightGBM", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°/ç·šå½¢å›å¸°", "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ", "XGBoost", "ARIMA", "Prophet"], default=default_models)
    
    if analysis_goal == "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰":
        tune_hyperparams = st.checkbox("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†", value=True)
        if problem_type != "æ™‚ç³»åˆ—äºˆæ¸¬" and len(models) > 1:
            use_ensemble = st.checkbox("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚’è¡Œã†", value=True)
        
    st.subheader("6. åˆ†æã¨å¯è¦–åŒ–")
    include_corr = st.checkbox("ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ", value=True)
    
    if problem_type == "æ™‚ç³»åˆ—äºˆæ¸¬":
        graph_options = ["æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã®ãƒ—ãƒ­ãƒƒãƒˆ", "æ™‚ç³»åˆ—åˆ†è§£å›³ (ãƒˆãƒ¬ãƒ³ãƒ‰, å­£ç¯€æ€§)", "ACF/PACFãƒ—ãƒ­ãƒƒãƒˆ", "ç‰¹å¾´é‡ã®é‡è¦åº¦ (SHAP)"]
        ts_features = st.multiselect("ä½œæˆã—ãŸã„æ™‚ç³»åˆ—ç‰¹å¾´é‡", ["æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ (å¹´, æœˆ, æ›œæ—¥ãªã©)", "ãƒ©ã‚°ç‰¹å¾´é‡", "ç§»å‹•å¹³å‡ç‰¹å¾´é‡", "ç¥æ—¥ç‰¹å¾´é‡"], default=["æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ (å¹´, æœˆ, æ›œæ—¥ãªã©)", "ãƒ©ã‚°ç‰¹å¾´é‡", "ç§»å‹•å¹³å‡ç‰¹å¾´é‡"])
    elif problem_type == "åˆ†é¡":
        graph_options = ["ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ (ã‚«ã‚¦ãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆ)", "ç‰¹å¾´é‡ã®é‡è¦åº¦ (SHAP)", "æ··åŒè¡Œåˆ—"]
    else: # å›å¸°
        graph_options = ["ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ (ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ )", "ç‰¹å¾´é‡ã®é‡è¦åº¦ (SHAP)", "å®Ÿç¸¾å€¤ vs äºˆæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ"]
    
    graphs = st.multiselect("ä½œæˆã—ãŸã„ã‚°ãƒ©ãƒ•ã®ç¨®é¡ï¼ˆSHAPã¯å¿…é ˆï¼‰", graph_options, default=graph_options)
    
    st.subheader("7. ä¿å­˜å…ˆ")
    save_dir_path = st.text_input("ã‚°ãƒ©ãƒ•ã‚„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€", "/content/drive/MyDrive/results/")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.header("âœ… ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
st.info("ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")

# âœ¨ FIX: `models`ãŒç©ºã§ãªã„ã“ã¨ã‚‚ãƒã‚§ãƒƒã‚¯æ¡ä»¶ã«è¿½åŠ 
if all([problem_type, target_col, source_type, data_context]) and models:
    generated_prompt_text = generate_prompt(
        problem_type, source_type, analysis_goal, data_context,
        train_path, test_path, submit_path, single_path,
        target_col, id_col, time_col,
        models, use_ensemble, tune_hyperparams,
        ts_features, include_corr, graphs,
        save_dir_path
    )
    st.text_area("ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", generated_prompt_text, height=600)
else:
    st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§å¿…é ˆé …ç›®ï¼ˆã‚´ãƒ¼ãƒ«ã€ç›®çš„ã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ãªã©ï¼‰ã‚’è¨­å®šãƒ»å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
